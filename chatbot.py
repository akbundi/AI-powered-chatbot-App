# -*- coding: utf-8 -*-
"""Untitled73.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZyhSWIzf8C9hRqdTElPcw1QfNIxr75iQ
"""

import os
import requests
from bs4 import BeautifulSoup
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# Set your Huggingface API key as an environment variable
os.environ["HUGGINGFACE_API_TOKEN"] = "your_huggingface_api_key_here"

# Function to fetch and scrape website content
def fetch_website_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        texts = soup.find_all(['h1', 'h2', 'h3', 'p'])
        content = " ".join(tag.get_text(strip=True) for tag in texts)
        return content
    else:
        raise Exception(f"Failed to fetch website content. Status code: {response.status_code}")

# Function to preprocess content
def preprocess_content(content):
    return content.replace("\n", " ").strip()

# Function to truncate input to fit within token limits
def truncate_input(user_query, website_content, tokenizer, max_tokens=3000):
    """
    Ensure the combined length of the user query and website content stays within max_tokens.
    """
    # Tokenize the user query
    user_query_tokens = tokenizer(user_query, return_tensors="pt", truncation=True)["input_ids"][0]
    user_query_length = len(user_query_tokens)

    # Reserve space for the user query and response tokens
    reserved_tokens = 100  # Reserve 100 tokens for the response
    max_content_tokens = max_tokens - user_query_length - reserved_tokens

    # Tokenize and truncate website content
    website_tokens = tokenizer(website_content, return_tensors="pt", truncation=False)["input_ids"][0]
    if len(website_tokens) > max_content_tokens:
        website_tokens = website_tokens[:max_content_tokens]

    # Decode truncated website content
    truncated_content = tokenizer.decode(website_tokens, skip_special_tokens=True)
    return user_query, truncated_content

# Chatbot interaction logic
def chatbot_interaction(model_pipeline, website_content, tokenizer):
    print("\nChatbot initialized. Type 'exit' to quit.\n")
    max_tokens = 3000  # Restrict to 3000 tokens

    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("\nChatbot session ended. Goodbye!\n")
            break

        # Truncate inputs to fit within token limits
        user_query, truncated_content = truncate_input(user_input, website_content, tokenizer, max_tokens)
        prompt = f"User Query: {user_query}\nRelevant Info: {truncated_content}\nResponse: "

        # Generate response
        response = model_pipeline(
            prompt,
            max_new_tokens=100,  # Limit response generation to 100 tokens
            num_return_sequences=1,
        )
        print(f"Bot: {response[0]['generated_text'].strip()}")

# Main function
def main():
    try:
        # Step 1: Fetch and preprocess website content
        url = "https://botpenguin.com/"
        print("Fetching website content...")
        raw_content = fetch_website_content(url)
        processed_content = preprocess_content(raw_content)
        print("Website content fetched and processed successfully.\n")

        # Step 2: Initialize Huggingface pipeline and tokenizer for GPT-NeoX
        print("Initializing Huggingface model...")
        model_pipeline = pipeline(
            "text-generation",
            model="EleutherAI/gpt-neox-20b",  # You can use any other GPT-NeoX model as needed
        )
        tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b", model_max_length=3000)  # Tokenizer for GPT-NeoX
        print("Model initialized successfully.\n")

        # Step 3: Start chatbot interaction
        chatbot_interaction(model_pipeline, processed_content, tokenizer)

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()